{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from generate_data import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math \n",
    "from load_matlab_timedata import get_data_from\n",
    "import sklearn.preprocessing \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "import yaml\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc_engine_random_sig_const_periods.slx\t\t diff_eq_system.slx\n",
      "dc_engine_random_sig_random_period.slx\t\t diff_eq_system.slx.original\n",
      "dc_engine_random_sig_random_period.slx.autosave  res.mat\n",
      "dc_engine_random_sig_random_period.slxc\t\t resources\n",
      "DC_motor_simulation.prj\t\t\t\t slprj\n"
     ]
    }
   ],
   "source": [
    "! ls DC_motor_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 20, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['ans']>\n"
     ]
    }
   ],
   "source": [
    "data = get_data_from('DC_motor_simulation/res.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILENAME = \"model_params.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(CONFIG_FILENAME, 'r') as f:\n",
    "#         config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KD: choosing simple value to avoid systematic error, as we have peacewise equal periods in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_nth = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time, x, y = data[::leave_nth, 0], data[::leave_nth, 1], data[::leave_nth, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert(x.shape == y.shape == time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = x.shape[0]\n",
    "num_datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To handle problems with equal splitting batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints -= num_datapoints % leave_nth\n",
    "num_datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func_timeseries(func, diapasone = (0, math.pi * 10, math.pi / 100)):\n",
    "    T = [ t for t in np.arange(diapasone[0], diapasone[1], diapasone[2])]\n",
    "    T = T[:num_datapoints]\n",
    "    func_t = [func(t) for t in T]\n",
    "    plt.plot(func_t)\n",
    "    x = np.array(T)\n",
    "    y = np.array(func_t)\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_sin = False\n",
    "if try_sin:\n",
    "    x, y = get_func_timeseries(func = math.sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normalized, x_norms = sklearn.preprocessing.normalize(x.reshape(-1,1),\n",
    "                                                  axis = 0,\n",
    "                                                  norm = 'max',\n",
    "                                                  return_norm = True)\n",
    "y_normalized, y_norms = sklearn.preprocessing.normalize(y.reshape(-1,1),\n",
    "                                                  axis = 0,\n",
    "                                                  norm = 'max', \n",
    "                                                  return_norm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_normalized\n",
    "y = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[num_datapoints//6:num_datapoints//5], label = \"in\")\n",
    "plt.plot(y[num_datapoints//6:num_datapoints//5], label = \"out\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of generating SIN wave of same len to werify model validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Set parameters\n",
    "#####################\n",
    "\n",
    "# Data params\n",
    "noise_var = 0\n",
    "num_datapoints = num_datapoints\n",
    "test_size = 0.3\n",
    "num_train = int((1-test_size) * num_datapoints)\n",
    "num_test = num_datapoints - num_train\n",
    "batch_size = 512\n",
    "# Network params\n",
    "input_size = 128\n",
    "# If `per_element` is True, then LSTM reads in one timestep at a time.\n",
    "per_element = True\n",
    "if per_element:\n",
    "    lstm_input_size = 1\n",
    "else:\n",
    "    lstm_input_size = input_size\n",
    "# size of hidden layers\n",
    "h1 = 16\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50000\n",
    "dtype = torch.float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size):\n",
    "    batch_start = random.randint(0, X.shape[1] - batch_size);\n",
    "    \n",
    "    x_batch = X.narrow(1,batch_start,batch_size)\n",
    "    \n",
    "    y_batch = y.narrow(0, batch_start,batch_size)\n",
    "    return (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batches = []\n",
    "for i in range(1,input_size+1):\n",
    "    x_batches.append(x[i:-(input_size+1-i)])\n",
    "x = np.array(x_batches)\n",
    "y = y[input_size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_batch, y_batch = get_batch(X_train, y_train, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x[input_size-3][:100], label = 'x_n_mimus_2')\n",
    "# plt.plot(x[input_size-2][:100], label = 'x_n_minus_1')\n",
    "# plt.plot(x[input_size-1][:100], label = 'x_n')\n",
    "# plt.plot(y[:100], label='y_n')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data = False\n",
    "#####################\n",
    "# Generate data\n",
    "#####################\n",
    "if generate_data:\n",
    "    from examples.generate_data import *\n",
    "    data = ARData(num_datapoints,\n",
    "                  num_prev=input_size,\n",
    "                  test_size=test_size, \n",
    "                  noise_var=noise_var,\n",
    "                  coeffs=fixed_ar_coefficients[input_size])\n",
    "\n",
    "    # make training and test sets in torch\n",
    "    X_train = torch.from_numpy(data.X_train).type(torch.Tensor)\n",
    "    X_test = torch.from_numpy(data.X_test).type(torch.Tensor)\n",
    "    y_train = torch.from_numpy(data.y_train).type(torch.Tensor).view(-1)\n",
    "    y_test = torch.from_numpy(data.y_test).type(torch.Tensor).view(-1)\n",
    "\n",
    "    X_train = X_train.view([input_size, -1, 1])\n",
    "    X_test = X_test.view([input_size, -1, 1])\n",
    "    plt.plot(data.X_train[:20, 12], label = '12')\n",
    "    plt.plot(data.X_train[:20, 16], label = '16')\n",
    "    plt.plot(data.X_train[:20, 19], label = '19')\n",
    "    plt.plot(data.y_train[:20], label = 'out')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(x[:, :num_train]).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = x[:, :num_train]\n",
    "# b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(x[:, :num_train]).type(torch.Tensor)\n",
    "# X_test = torch.from_numpy(x[:, num_train:]).type(torch.Tensor)\n",
    "X_train = X_train.view([input_size, -1, 1])\n",
    "# X_test = X_test.view([input_size, -1, 1])\n",
    "y_train = torch.from_numpy(y[:num_train, :]).type(torch.Tensor).view(-1)\n",
    "# y_test = torch.from_numpy(y[num_train:, :]).type(torch.Tensor).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = np.zeros_like(x[:, :num_train])\n",
    "tst[:,:x.shape[1] - num_train].shape\n",
    "tst[:,:x.shape[1] - num_train] += x[:, num_train:]\n",
    "X_test = torch.from_numpy(tst).type(torch.Tensor)\n",
    "X_test = X_test.view([input_size, -1, 1])\n",
    "y_tst = np.zeros_like(y[:num_train, :])\n",
    "y_tst[:y.shape[0] - num_train, :] = y[num_train:, :]\n",
    "y_test = torch.from_numpy(y_tst).type(torch.Tensor).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.detach().numpy()[input_size-2])\n",
    "plt.plot(y_train.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "import sklearn.preprocessing \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Loads dataset from matlab file and provides batching interface\"\"\"\n",
    "\n",
    "    def __init__(self, mat_file, retrospective_steps, partition, need_normalize=True, leave_nth = 1, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mat_file (string): Path to the mat file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            \n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "            \n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        outfile = h5py.File(mat_file, 'r')\n",
    "        #print(outfile.keys())\n",
    "        self.data  = outfile['ans']\n",
    "        time, x, y = self.data[::leave_nth, 0], self.data[::leave_nth, 1], self.data[::leave_nth, 2]\n",
    "        if need_normalize:\n",
    "            x_normalized, self.x_norms = sklearn.preprocessing.normalize(x.reshape(-1,1),\n",
    "                                                      axis = 0,\n",
    "                                                      norm = 'max',\n",
    "                                                      return_norm = True)\n",
    "            y_normalized, self.y_norms = sklearn.preprocessing.normalize(y.reshape(-1,1),\n",
    "                                                      axis = 0,\n",
    "                                                      norm = 'max', \n",
    "                                                      return_norm = True)\n",
    "            x = x_normalized\n",
    "            y = y_normalized\n",
    "            \n",
    "        print(x.shape)\n",
    "        x = x[ int(partition[0] * x.shape[0]) : int(partition[1] * x.shape[0])]\n",
    "        print(x.shape)\n",
    "        y = y[ int(partition[0] * y.shape[0]) : int(partition[1] * y.shape[0])]\n",
    "        x_sliding = []  # determines number of steps for retrospective view\n",
    "        for i in range(1,retrospective_steps+1):\n",
    "            x_sliding.append(x[i:-(retrospective_steps+1-i)])\n",
    "        y = y[retrospective_steps:]\n",
    "        #print(y.shape)\n",
    "        self.x = torch.from_numpy(np.array(x_sliding)).type(torch.Tensor)\n",
    "        print(\"before\")\n",
    "        print(self.x.shape)\n",
    "        \n",
    "        self.x = self.x.view([input_size, -1, 1])\n",
    "        print(\"after\")\n",
    "        print(self.x.shape)\n",
    "        self.y = torch.from_numpy(y).type(torch.Tensor).view(-1)\n",
    "        #print(self.y.shape)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[:, idx], self.y[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts_ds = TimeSeriesDataset(mat_file='DC_motor_simulation/res.mat',\n",
    "                                retrospective_steps=input_size,\n",
    "                                partition=(0, 1 - test_size),\n",
    "                                need_normalize=True, leave_nth=13)\n",
    "test_ts_ds = TimeSeriesDataset(mat_file='DC_motor_simulation/res.mat',\n",
    "                                retrospective_steps=input_size,\n",
    "                                partition=(1 - test_size, 1),\n",
    "                                need_normalize=True, leave_nth=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_ts_ds))\n",
    "len(train_ts_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_ts_ds))\n",
    "len(train_ts_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset = train_ts_ds,\n",
    "                 batch_sampler = BatchSampler(sampler = SequentialSampler(train_ts_ds), \n",
    "                              batch_size = batch_size, drop_last = True))\n",
    "test_dl = DataLoader(dataset = test_ts_ds,\n",
    "                 batch_sampler = BatchSampler(sampler = SequentialSampler(test_ts_ds), \n",
    "                              batch_size = batch_size, drop_last = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dl))\n",
    "print(len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dl))\n",
    "print(len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_dl:\n",
    "    x, y, ind = i\n",
    "    print(x.shape)\n",
    "    x = x.transpose(0,1)\n",
    "    plt.plot(x.detach().numpy()[-2])\n",
    "    plt.plot(y.detach().numpy())\n",
    "    print(len(x))\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        # Only take the output from the final timestep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        \n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1)\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        return sum([torch.prod(torch.Tensor(list(p.size()))) for p in model_parameters]).item()\n",
    "\n",
    "model = LSTM(lstm_input_size,\n",
    "             h1,\n",
    "             batch_size=batch_size,\n",
    "             output_dim=output_dim,\n",
    "             num_layers=num_layers)\n",
    "\n",
    "# loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "loss_fn = torch.nn.L1Loss(reduction=\"sum\")\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiscale(plot_requests):\n",
    "        fig, ax = plt.subplots()\n",
    "        # Twin the x-axis twice to make independent y-axes.\n",
    "        for i, req in enumerate(plot_requests):\n",
    "            d, c = req\n",
    "            ax.twinx().plot(d, color = c)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shared_scale(plot_requests):\n",
    "    for d, l in plot_requests:\n",
    "        plt.plot(d, label = l)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "num_epochs = 10\n",
    "model.hidden = model.init_hidden()\n",
    "hist = {\"train_loss\":np.zeros(num_epochs), \n",
    "        \"test_loss\":np.zeros(num_epochs),\n",
    "       \"train_local_loss\":np.zeros(len(train_dl)),\n",
    "       \"test_local_loss\":np.zeros(len(test_dl))}\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    for i, data in enumerate(tqdm(train_dl)):\n",
    "        x_batch, y_batch, indexes = data\n",
    "        x_batch.transpose_(0, 1)\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    #model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "        #y_pred, train_loss = train_on_batch(model, x_batch, y_batch)\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        hist['train_local_loss'][i] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    plot_shared_scale(\n",
    "        [(y_pred.detach().numpy(),\"Preds\" ),\n",
    "         (y_batch.detach().numpy(), \"Data\"),\n",
    "         (x_batch[-1].detach().numpy(), \"Input\")])\n",
    "    residuals = y_batch-y_pred\n",
    "    plot_multiscale([(residuals.detach().numpy(), \"Red\"),\n",
    "                          (y_batch.detach().numpy(), \"Green\")]) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        # validate during evaluation\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for i, data in enumerate(tqdm(test_dl)):\n",
    "            x_batch_test, y_batch_test, indexes = data\n",
    "            x_batch_test.transpose_(0, 1)\n",
    "            y_pred_test = model(x_batch_test)\n",
    "            loss = loss_fn(y_pred_test, y_batch_test)\n",
    "            hist['test_local_loss'][i] = loss.item()\n",
    "        model = model.train()\n",
    "        \n",
    "    plot_shared_scale([\n",
    "            (y_pred_test.detach().numpy(),\"Preds\" ),\n",
    "             (y_batch_test.detach().numpy(), \"Data\"),\n",
    "             (x_batch_test[-1].detach().numpy(), \"Input\")])\n",
    "\n",
    "    residuals = y_batch_test - y_pred_test\n",
    "    plot_multiscale(\n",
    "            [(residuals.detach().numpy(), \"Red\"), \n",
    "             (y_batch_test.detach().numpy(),\"Green\")])\n",
    "    plot_shared_scale( [ (hist[\"train_local_loss\"], \"train\"),\n",
    "                        (hist[\"test_local_loss\"], \"test\")])\n",
    "    hist[\"train_loss\"][t] = np.average(hist[\"train_local_loss\"])\n",
    "    hist[\"test_loss\"][t] = np.average(hist[\"test_local_loss\"])\n",
    "    print(f\"Epoch [{t}] train_loss[{hist['train_loss'][t]}] test_loss[{hist['test_loss'][t]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist['test_loss'])\n",
    "plt.plot(hist['train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_train = []\n",
    "smoothed_test = []\n",
    "k_smooth = 50\n",
    "for elm in range(hist['train_loss'][:-k_smooth].shape[0]):\n",
    "    smoothed_train.append(np.average(hist['train_loss'][elm:elm+k_smooth]))\n",
    "for elm in range(hist['test_loss'][:-k_smooth].shape[0]):\n",
    "    smoothed_test.append(np.average(hist['test_loss'][elm:elm+k_smooth]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smoothed_train[10000:15000])\n",
    "plt.plot(smoothed_test[10000:15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist['train_loss'][:], label=\"train\")\n",
    "plt.plot(hist['test_loss'][:], label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:, :test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist['train_loss'][1000:], label=\"train\")\n",
    "plt.plot(hist['test_loss'][1000:], label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist['test_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    pass #piece of code preventing from overwriting previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO place it in the training loop for stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO provide config and experiment loggiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "plt.plot(y_pred_test[0:800].detach().numpy(), label=\"Preds\")\n",
    "plt.plot(y_test[0:800].detach().numpy(), label=\"Data\")\n",
    "plt.plot(X_test[input_size-1][0:800].detach().numpy(), label=\"Input\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "residuals = y_test-y_pred_test\n",
    "plot_multiscale(data=[residuals.detach().numpy(), y_test.detach().numpy()], \n",
    "       colors = [\"Red\", \"Green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
